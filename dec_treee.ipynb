import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
import os
from pydotplus import graph_from_dot_data
from sklearn.tree import export_graphviz
from sklearn.metrics import accuracy_score

# Loading dataset
df = pd.read_csv('Iris.csv')  # Read the Iris dataset CSV
df.drop('Id', axis=1, inplace=True)  # Drop the 'Id' column which is not necessary for classification

# Separating features (X_) and target (Y_)
X_ = df.iloc[:, :-1]  # All rows, excluding the last column (features)
Y_ = df.iloc[:, -1]  # All rows, last column (target - species)

# Print the dataset and its shape
print(df)
print(f"Shape of the dataset: {df.shape}")

# Splitting data into training and testing sets (70% training, 30% testing)
X_train, X_test, Y_train, Y_test = train_test_split(X_, Y_, test_size=0.3, random_state=1)

# Print the shape of training and testing sets
print(f"Training features shape: {X_train.shape}")
print(f"Testing features shape: {X_test.shape}")

# Adding Graphviz to the system path for visualization
os.environ["PATH"] += os.pathsep + r"C:\Program Files\Graphviz\bin"

# Setting minimum samples required to split a node
min_sample_ = 30

# Initializing the DecisionTreeClassifier with 'entropy' as the splitting criterion
tree = DecisionTreeClassifier(criterion='entropy', 
                              min_samples_split=min_sample_,  # Minimum samples required to split a node
                              random_state=1)  # Setting random_state for reproducibility

# Training the decision tree on the training set
tree.fit(X_train, Y_train)

# Exporting the trained decision tree to DOT format
dot_data = export_graphviz(tree,
                           filled=True,  # Fill the nodes with colors to indicate classes
                           rounded=True,  # Make the nodes rounded
                           class_names=['Setosa', 'Versicolor', 'Virginica'],  # Class names in the target
                           feature_names=['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm'],  # Feature names
                           out_file=None)  # Do not write to a file, keep it in a string

# Converting the DOT data to a graph
graph = graph_from_dot_data(dot_data)

# Saving the graph as a PNG file
graph.write_png('split_tree_{}.png'.format(min_sample_))  # Save as PNG image with dynamic file name based on min_samples_split

# Display the plot showing the tree's structure
plt.imshow(plt.imread('split_tree_{}.png'.format(min_sample_)))
plt.axis('off')  # Turn off axes
plt.show()

# Initialize the DecisionTreeClassifier with entropy criterion and min_samples_split parameter
tree = DecisionTreeClassifier(criterion='entropy',
                              min_samples_split=min_sample_,
                              random_state=1)

# Fit the model on the training data
tree.fit(X_train, Y_train)

# Export the trained decision tree model to DOT format
dot_data = export_graphviz(tree,
                           filled=True,               # Nodes will be filled with colors
                           rounded=True,              # Rounded corners for the nodes
                           class_names=['Setosa', 'Versicolor', 'Virginica'],  # Target class names
                           feature_names=['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm'],  # Feature names
                           out_file=None)  # Do not save to file, return the data as a string

# Convert the DOT data to a graph object
graph = graph_from_dot_data(dot_data)

# Write the graph to a PNG file with the name based on min_sample_
graph.write_png('decision_tree_{}.png'.format(min_sample_))

# Initial prediction with the previously trained tree (for reference)
predict_ = tree.predict(X_test)

# List to store accuracy for different tree depths
y = []

# Loop through different values of max_depth for the decision tree
for i in range(1, 5):
    # Train a new decision tree with 'entropy' criterion and varying max_depth
    tree = DecisionTreeClassifier(criterion='entropy', max_depth=i, random_state=1)
    tree.fit(X_train, Y_train)  # Fit the model to the training data
    predict_ = tree.predict(X_test)  # Make predictions on the test set

    # Calculate accuracy and append it to the list 'y'
    acc = accuracy_score(Y_test, predict_) * 100  # Multiply by 100 to get percentage
    y.append(acc)

# Plotting the accuracy for different values of max_depth
plt.plot(range(1, 5), y, '-o', c='blue')  # Plot accuracy vs tree depth
plt.title("Accuracy of Decision Tree Classifier with Varying Depth")
plt.xlabel("Max Depth")
plt.ylabel("Accuracy (%)")
plt.show()
